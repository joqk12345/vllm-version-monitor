# vLLM 每日报告

## 日期：2026-02-26

## 版本信息

### PyPI 版本：0.15.1

### GitHub 发布版本：v0.15.1

## 版本对比

从 0.0.0 更新到 v0.15.1

## 每日提交

- 1976356ee69750630189eb127fc9eeaa6f8e0c9e - [MoE Refactor] MXFP4 Cutlass Experts to MK (#34542)

Signed-off-by: Yongye Zhu <zyy1102000@gmail.com> (2026-02-26 01:32:39)
- cbf8f7028cc0d80de4eeaf789b4bd56afbb5aafd - [UX] Add `--performance-mode {balanced,interactivity,throughput}` (#34936)

Signed-off-by: mgoin <mgoin64@gmail.com> (2026-02-26 01:28:31)
- 6831650c40ac3a34f049e285d9ad6b87daddbe00 - [offloader] v2: Hide weight onloading latency via prefetching (#29941)

Signed-off-by: Ming Yang <minos.future@gmail.com>
Signed-off-by: Michael Goin <mgoin64@gmail.com>
Co-authored-by: Michael Goin <mgoin64@gmail.com> (2026-02-26 01:20:59)

## 热门 Issues 和 PRs

- #20859: [Feature] limit thinking tokens (hard limit) (51 条评论)
- #29184: [Core] NGram GPU Implementation compatible with Async Scheduler (26 条评论)
- #29117: [torch.compile] refactor config hashing to compile_factors and unify factor collection (44 条评论)
- #20802: [Model] Add support for Jina Embeddings   V4 (16 条评论)
- #31941: LoRA Per Request Loading Pipelining Support (4 条评论)

## 功能更新

待实现

## Bug 修复

待实现

## 性能改进

待实现

---

*Generated by vLLM Version Monitor Skill*