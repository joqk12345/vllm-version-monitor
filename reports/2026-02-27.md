# vLLM 每日报告

## 日期：2026-02-27

## 版本信息

### PyPI 版本：0.16.0

### GitHub 发布版本：v0.16.0

## 版本对比

从 0.0.0 更新到 v0.16.0

## 每日提交

- 4fec53cfcb3135f223ce52d2c6b7bbf9ddf2be63 - [CI] Actually run tests/kernels/quantization/test_block_fp8.py in CI (#34274) (2026-02-27 00:58:03)
- 38c498b8e3aaec95049f384edfc56ca12cbe1839 - [Performance] Cublas Bf16 Gate with Fp32 Output (#35121)

Signed-off-by: Roi Koren <roik@nvidia.com> (2026-02-27 00:51:28)
- 56a6371706bc32c9e2e996ec29911c087a547ac0 - [Update] Use FlashInfer fast_decode_plan directly instead of replication (#34687)

Signed-off-by: Andrii <askliar@nvidia.com>
Co-authored-by: Andrii <askliar@nvidia.com> (2026-02-27 00:31:43)

## 热门 Issues 和 PRs

- #20859: [Feature] limit thinking tokens (hard limit) (52 条评论)
- #29184: [Core] NGram GPU Implementation compatible with Async Scheduler (26 条评论)
- #29117: [torch.compile] refactor config hashing to compile_factors and unify factor collection (44 条评论)
- #20802: [Model] Add support for Jina Embeddings   V4 (16 条评论)
- #31941: LoRA Per Request Loading Pipelining Support (6 条评论)

## 功能更新

待实现

## Bug 修复

待实现

## 性能改进

待实现

---

*Generated by vLLM Version Monitor Skill*